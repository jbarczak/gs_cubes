
Let's look at a simple, constrained test case.   Suppose I want to render a very large number of oriented boxes.  That makes for a rather boring scene, yes, but there are legitimate use cases in which I might want to do it.   I might have a deferred renderer that's rendering light bounding volumes, or maybe I'm doing an object space AO technique like Oat, Shopf, and I published in ShaderX7  "Deferred Occclusion From Analytic Surfaces".   Or, maybe I'm rendering lots of AABBs for occlusion culling.   Let's suppose for a moment that this is thing I'm interested in.  How do I do this as fast as possible?


For our experiments, we'll render the boxes using a simple pixel shader the uses derivative trickery to extract a normal, and then does diffuse lighting.  This is only really to give us something interesting to see and to make sure that we didn't mangle the geometry:

<code>

All of the code can be found here: <LINK>


Let's consider three different ways of submitting our boxes. 

Method 1:  The obvious way.


An oriented box can be defined by a 3x4 transform matrix which deforms an ordinary unit cube into the box we're looking for:
<matrix>


Let's just render a bunch of instanced boxes.   We have one unit box mesh, and a buffer packed full of transform matrices, and we use an ordinary instanced drawcall.  Our vertex shader looks like this:


I'm using float4s and dot products instead of a float3x4 matrix type, because TBH I can never keep N and M straight in my head, and the dot products are easier on my brain.  

Method 2:  The "I hate Instancing" way.

Let's do the same thing, except let's not use instancing.   The reasons for this will become clear shortly.  Now, we don't want to go doing one drawcall per box, because that would just introduce more overhead.  Instead, we'll do this by generating a gigantic index buffer and doing SV_VertexID math.   We know that cube i will reference vertices 8*i through 8*i+7, so we can figure out our own local instance and vertex ID from a flat index buffer.   The only drawback is that now we need to fetch our vertex and instance data explicitly:

<code>

Method 3:  The Clever Way.

Let's think outside the box about our boxes for a second.   What is it that we have to do in order to draw a box?   We need to compute the clip space positions of each of its 8 vertices.  That's all.   So far, we've done this by doing a pair of matrix multiplications.  We deform a unit cube into our oriented box, and then apply our view-projection matrix to the result.  Like so:

<math>

If we account for the fact that our vertex coordinates are all +-1, then we can boil the world matrix down to a series of vector adds and subtracts, like so:

<math>

Now we can exploit the distributive property of matrix multiplication and factor out the view-projection transform, like so:

<math>

The geometric interprettation of this is that instead of expanding our box in world-space, and then transforming the results to clip space, we instead do the expansion in clip space.

The final piece of the puzzle is to minimize the number of verts that our GS emits.  Some clever individual figured out how to represent a cube using a single triangle strip (14 vertices).  See: http://www.asmcommunity.net/forums/topic/?id=6284.  That's a considerable improvement over the 24 verts we might emit if we did it 2 triangles at a time.

Here is the full geometry shader:

<code>


Analysis. 

Let's ask ourselves which approach is better.  To start with, let's look at the shaders and count instructions.  

Here is the DX bytecode for our instanced vertex shader:
vs_5_0
dcl_globalFlags refactoringAllowed
dcl_constantbuffer cb0[4], immediateIndexed
dcl_input v0.xyzw
dcl_input v1.xyzw
dcl_input v2.xyzw
dcl_input v3.xyzw
dcl_output_siv o0.xyzw, position
dcl_temps 2
dp4 r0.x, v0.xyzw, v2.xyzw
mul r0.xyzw, r0.xxxx, cb0[1].xyzw
dp4 r1.x, v0.xyzw, v1.xyzw
mad r0.xyzw, r1.xxxx, cb0[0].xyzw, r0.xyzw
dp4 r1.x, v0.xyzw, v3.xyzw
mad r0.xyzw, r1.xxxx, cb0[2].xyzw, r0.xyzw
add o0.xyzw, r0.xyzw, cb0[3].xyzw
ret 

If we count flops, we find that our vertex shader contains 28 flops.  A dp4 is a mul followed by 3 mads, and a mad is one flop regardless of what marketing people would like you to believe.  Multiply that by 8, and we get a total of 224 flops per box to transform a vertex.  We've only got 8 verts so the post TnL cache will handle the duplication.

Now let's look at the geometry shader:

gs_5_0
dcl_globalFlags refactoringAllowed
dcl_constantbuffer cb0[4], immediateIndexed
dcl_input v[1][0].xyzw
dcl_input v[1][1].xyzw
dcl_input v[1][2].xyzw
dcl_temps 7
dcl_inputprimitive point 
dcl_stream m0
dcl_outputtopology trianglestrip 
dcl_output_siv o0.xyzw, position
dcl_maxout 14
mul r0.xyzw, cb0[1].xyzw, v[0][1].wwww
mad r0.xyzw, v[0][0].wwww, cb0[0].xyzw, r0.xyzw
mad r0.xyzw, v[0][2].wwww, cb0[2].xyzw, r0.xyzw
add r0.xyzw, r0.xyzw, cb0[3].xyzw
mul r1.xyzw, cb0[1].xyzw, v[0][1].xxxx
mad r1.xyzw, v[0][0].xxxx, cb0[0].xyzw, r1.xyzw
mad r1.xyzw, v[0][2].xxxx, cb0[2].xyzw, r1.xyzw
add r2.xyzw, r0.xyzw, r1.xyzw
add r0.xyzw, r0.xyzw, -r1.xyzw
mul r1.xyzw, cb0[1].xyzw, v[0][1].zzzz
mad r1.xyzw, v[0][0].zzzz, cb0[0].xyzw, r1.xyzw
mad r1.xyzw, v[0][2].zzzz, cb0[2].xyzw, r1.xyzw
add r3.xyzw, r1.xyzw, r2.xyzw
add r2.xyzw, -r1.xyzw, r2.xyzw
mul r4.xyzw, cb0[1].xyzw, v[0][1].yyyy
mad r4.xyzw, v[0][0].yyyy, cb0[0].xyzw, r4.xyzw
mad r4.xyzw, v[0][2].yyyy, cb0[2].xyzw, r4.xyzw
add r5.xyzw, r3.xyzw, r4.xyzw
add r3.xyzw, r3.xyzw, -r4.xyzw
mov o0.xyzw, r5.xyzw
emit_stream m0
add r6.xyzw, r0.xyzw, r1.xyzw
add r0.xyzw, r0.xyzw, -r1.xyzw
add r1.xyzw, r4.xyzw, r6.xyzw
add r6.xyzw, -r4.xyzw, r6.xyzw
mov o0.xyzw, r1.xyzw
emit_stream m0
mov o0.xyzw, r3.xyzw
emit_stream m0
mov o0.xyzw, r6.xyzw
emit_stream m0
add r6.xyzw, -r4.xyzw, r0.xyzw
add r0.xyzw, r4.xyzw, r0.xyzw
mov o0.xyzw, r6.xyzw
emit_stream m0
mov o0.xyzw, r1.xyzw
emit_stream m0
mov o0.xyzw, r0.xyzw
emit_stream m0
mov o0.xyzw, r5.xyzw
emit_stream m0
add r1.xyzw, r2.xyzw, r4.xyzw
add r2.xyzw, r2.xyzw, -r4.xyzw
mov o0.xyzw, r1.xyzw
emit_stream m0
mov o0.xyzw, r3.xyzw
emit_stream m0
mov o0.xyzw, r2.xyzw
emit_stream m0
mov o0.xyzw, r6.xyzw
emit_stream m0 
mov o0.xyzw, r1.xyzw
emit_stream m0
mov o0.xyzw, r0.xyzw
emit_stream m0
ret 

If we look at just the math, our total here is 108 flops per box.  I'm going to assume that the driver will get rid of those idiotic movs, but it's hard to tell how much those 'emit' things actually cost.  Still, we're only emitting 56 dwords, so even if its around one 'flop' per dword, we're still in reasonable shape, or so we think.  Let's see how things play out....

Let's look at the numbers now:

<chart>

nv gtx670:
2.06ms
0.95ms
1.05ms  -90%

amd a10:
13.6ms
11.3ms
16.4ms - 68%

my hsw: 
32ms
9.8ms
3.9ms

The first thing we notice is that instancing sucks. If you are rendering a very small number of verts per instance, you want to do it without instancing, because all three geometry pipelines take a hit.  Especially those poor blue fellas. This result is just plain irritating.  In order to avoid whatever silly bottleneck this is, it's necessary for us to create a redundant index buffer and add a few spurious instructions to the vertex shader.  It's counter-intuitive, and a tad wasteful.   

The second thing we notice is that our GS idea was not as clever as we thought. On both the AMD and Nvidia parts, our clever idea, which was supposed to cut our workload in half, has instead hurt us.  To understand why, let's dig a little deeper.

*****************************
Why Geometry Shaders Suck:
*****************************

We need to see some actual shader assembly, but unfortunately, only one vendor lets us do this.  So, I'll do what I do during my day job.  I will assume that everybody's hardware is exactly the same as GCN, and make all of my shader design decisions accordingly.  If the blue and green teams are made nervous by that, then perhaps they should start listening to my incessant whining.

By looking at the disassembly we can see something interesting.  The shader is writing all of its output to memory. That's right.  Every vertex we emit from an AMD geometry shader has to make a round-trip through memory.  


Why do you suppose they're doing this?

The API requires that the output of a geometry shader be rendered in input order.   The fixed-function hardware on the other side is required to consume geometry shader outputs serially.  This creates a sync point.  If we want to process multiple primitives in parallel, it is necessary for GS instances to buffer up their outputs so that they can be fed in the correct order to whoever is consuming them.  The more parallelism, and the more verts our GS emits, the more buffering we need. 

Recall that GPU shader pipelines operate in SIMD fashion.  The amount of buffering we need is proportional to the SIMD width.  AMD's SIMD is 64 threads wide, which means that in our case they must buffer 14336 bytes for every GS wave.  For Nvidia, it's 7168 bytes per warp.  On a state of the art R9 with 40 CUs, we need at least 160 waves just to keep all of the schedulers occupied, which translates to over 2MB of buffering, and you'll still need more than that because 1 per SIMD just ain't enough.

There are only two places this buffering can exist.  It's either on chip, in a cache somewhere, or its off chip, in DRAM.  If you put it on chip, you need to throttle the number of concurrent warps based on the amount of space you have, and if you put it off chip, it's going to take that much longer for the consumer to finish it, which means that unless the shader is insanely expensive there is no way you'll be able to avoid being stalled on it.  Back in the DX10 era, Nvidia went the on-chip route, and AMD went the off-chip route.  I don't think that either is particularly happy with the results.

The vertex shader, even though it does 2x as much work, only produces 16 bytes per thread, which translates to 1024 bytes/wave (AMD), or 512 (nv). By using a GS, we replace a large number of low-bandwidth threads with a small number of high bandwidth threads, and even though we perform less work, we're not able to parallelize it as well.


******************************
Why Intel's Geometry Shaders Suck Far Less
******************************

After a thorough perusal of their linux graphics docs, it can be shown that their works by blocking threads.  Each thread generates its output, puts in registers, and waits its turn to feed it downstream.  If an EU has a GS thread that is blocked at the sync point, it can start executing another GS thread while the first one is waiting.  As long as the GS threads are doing a goodly amount of computation, the machine stays busy.  I speculate that Intel gets away with this for two reasons:

1.  Unlike the competition, Intel's shader hardware has a fixed number of hardware threads, each of which gets its own full set of registers.  The red and green team each lose thread occupancy if a shader has a lot of register pressure, but not the blue team, they just exploit their ridiculous process advantage and pack the little suckers in, and then stop worrying about it.  Our shader has quite a bit of register pressure in it, but that doesn't hurt Intel's concurrency one bit.  Their enormous register file functions as buffer space.

2.  Intel's shader EU's are interesting in that they can operate in a variety of modes.  They're 8 wide, and can run in SIMD-8 mode (where each 8 threads issue one operation), or SIMD16 mode (where 16 threads issue from back to back registers), or SIMD4x2 mode, where 2 threads each issue 4 operations.  SIMD4x2 is used in Haswell for VS and GS, and I suspect that its the main reason for the awesomeness.  Intel is only running two GS invocations at a time, and is replacing data-parallelism with instruction level parallelism, which means that their per-warp output need is a measly 432 bytes, which is an order of magnitude lower than everybody else's.  So, not only can intel run a lot more GS threads than everybody else, but they also don't need nearly as many of them, because it takes less time to consume 432 bytes than 14336.  So, Intel wins, but only because of its more flexible architecture.

I still seriously dislike the geometry shader, at least in its present form.  It injects a high bandwidth operation into a sensitive, serialized area of the pipeline.  However, this little exercise has made me wonder if it can't be redeemed.  Perhaps it could be redesigned to include a 'fork' phase, so that shaders like this one can be threaded at a courser granularity than one primitive per warp.  This shader is almost entirely mads, so maybe there's a way to refactor my shader so that the calculations for a single instance can be spread across a warp instead of having one full primitive in every thread.  The driver compilers probably aren'tup to the task, but I certainly would be, if it were an important enough shader.  Unfortunately, even if I did find a way to express the shader this way, it's still not possible for me to USE such a shader as a GS.  The APIs don't permit it.  In the future, perhaps we want a lower level model there, something like, "Here are N compute threads responsible for M primitives.  Go"   Food for thought, anyway...






